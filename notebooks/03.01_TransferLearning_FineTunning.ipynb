{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqg4y371CM7X"
   },
   "source": [
    "# Experiments - Fine tune Bert\n",
    "\n",
    "The goal of this notebook is use [Building a Sentiment Corpus of Tweets in Brazilian Portuguese](https://arxiv.org/abs/1712.08917)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31OW0dhozvli"
   },
   "source": [
    "## Libraries and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdy party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import funcy as fp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization / Presentation\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "# Model Training and Evaluation\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.pardir))\n",
    "\n",
    "from src import settings\n",
    "from src.pipeline.resources import load_corpus\n",
    "from src.models.transformer import preprocess, initialize_model, set_seed, get_device, evaluate, predict\n",
    "from src.utils import format_nested_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "pd.set_option('max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 3\n",
    "EXPERIMENT_RUN_NAME = f'03_TransferLearning-FineTuning-FlexibleLayers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u07WRKnxsX96"
   },
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3880,
     "status": "ok",
     "timestamp": 1604425976287,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "q0wnL46x_u-D"
   },
   "outputs": [],
   "source": [
    "frame = load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 3840,
     "status": "ok",
     "timestamp": 1604425976317,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "79T-9rhG_3NN",
    "outputId": "27e068ef-545e-40d4-9c95-8578ca2e224f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3905</th>\n",
       "      <td>USERNAME sophia abrah√£o como foi a participa√ß√£o do s√©rgio malheiros</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>O VITOR FOI EMBORAAA N√ÉO ACREDITO AHSIAHSIJANXOABFNOSNFNSONRV</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11733</th>\n",
       "      <td>que lindinha que ela t√° de unic√≥rnio</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4162</th>\n",
       "      <td>se toda vez que um homem ajuda nas coisas de casa ser vangloriado isso nunca vai se tornar normal #Encontro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>ol√° USERNAME gostaria da participa√ß√£o do fiuk em algum quadro no USERNAME . obrigada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9113</th>\n",
       "      <td>eu sou o cara que come um dog√£o depois da balada sempre que poss√≠vel eu sou esse cara tamb√©m üòÇ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              text  \\\n",
       "3905                                           USERNAME sophia abrah√£o como foi a participa√ß√£o do s√©rgio malheiros   \n",
       "5386                                                 O VITOR FOI EMBORAAA N√ÉO ACREDITO AHSIAHSIJANXOABFNOSNFNSONRV   \n",
       "11733                                                                         que lindinha que ela t√° de unic√≥rnio   \n",
       "4162   se toda vez que um homem ajuda nas coisas de casa ser vangloriado isso nunca vai se tornar normal #Encontro   \n",
       "1061                          ol√° USERNAME gostaria da participa√ß√£o do fiuk em algum quadro no USERNAME . obrigada   \n",
       "9113                eu sou o cara que come um dog√£o depois da balada sempre que poss√≠vel eu sou esse cara tamb√©m üòÇ   \n",
       "\n",
       "       label  group  \n",
       "3905     1.0  train  \n",
       "5386     0.0  train  \n",
       "11733    2.0  train  \n",
       "4162     0.0  train  \n",
       "1061     1.0  train  \n",
       "9113     2.0  train  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = (frame\n",
    "         .assign(label=lambda f: f['sentiment'].map({'-1': 0, '0':1, '1': 2}))\n",
    "         [['text', 'label', 'group']]\n",
    "        )\n",
    "frame.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3829,
     "status": "ok",
     "timestamp": 1604425976319,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "X4HKAFTbvMwI",
    "outputId": "d0415abd-ea6e-4b49-b279-2dcc0e96ec62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12990 | Test: 2010\n"
     ]
    }
   ],
   "source": [
    "train_frame = frame.loc[lambda f: f['group'] == 'train']\n",
    "test_frame = frame.loc[lambda f: f['group'] == 'test']\n",
    "del frame\n",
    "\n",
    "X_test = test_frame.text.values\n",
    "y_test = test_frame.label.values\n",
    "\n",
    "X_train = train_frame.text.values\n",
    "y_train = train_frame.label.values\n",
    "\n",
    "print(f'Train: {len(X_train)} | Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X79dYY3sxDCi"
   },
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3820,
     "status": "ok",
     "timestamp": 1604425976320,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "K7hxtI4l0SUJ",
    "outputId": "a4fdb62b-3287-4cf9-ab21-585ccda36811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) available: 1. Device name: GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'GPU(s) available: {torch.cuda.device_count()}. Device name: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('Using the CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEPPYHa62JXF"
   },
   "source": [
    "## Fine Tuning Model\n",
    "\n",
    "### Define Parameters and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel,  BertTokenizer\n",
    "\n",
    "MODEL_CLASS = BertModel\n",
    "MODEL_TOKENIZER = BertTokenizer\n",
    "#MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
    "MODEL_NAME = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "MODEL_CLASS = RobertaModel\n",
    "MODEL_TOKENIZER = RobertaTokenizer\n",
    "\n",
    "MODEL_NAME = 'rdenadai/BR_BERTo'\n",
    "\"\"\"\n",
    "\n",
    "MODEL_LAYERS = []\n",
    "MODEL_DROPOUT_LAYERS = []\n",
    "\n",
    "\"\"\"\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "MODEL_CLASS = RobertaModel\n",
    "MODEL_TOKENIZER = RobertaTokenizer\n",
    "MODEL_NAME = 'rdenadai/BR_BERTo'\n",
    "\"\"\"\n",
    "\n",
    "FREEZE = False\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "SEED = 42\n",
    "\n",
    "preprocessing_params = {\n",
    "    'unify_html_tags': True,\n",
    "    'unify_urls': True, \n",
    "    'trim_repeating_spaces': True,\n",
    "    'unify_hashtags': False, \n",
    "    'unify_mentions': True,\n",
    "    'unify_numbers': False, \n",
    "    'trim_repeating_letters': True,\n",
    "    'lower_case': True\n",
    "}\n",
    "\n",
    "tokenizer = MODEL_TOKENIZER.from_pretrained(MODEL_NAME, do_lower_case=preprocessing_params['lower_case'])\n",
    "preprocessing_params['tokenizer'] = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNE9oASMZ1bN"
   },
   "source": [
    "Encode all sentences to get the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29513,
     "status": "ok",
     "timestamp": 1604426002286,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "hrbvKGNAlMtt",
    "outputId": "38087981-c897-4f40-ad58-b50c21c2003f"
   },
   "outputs": [],
   "source": [
    "all_tweets = np.concatenate([train_frame.text.values, test_frame.text.values])\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "preprocessing_params['max_len'] = max_len\n",
    "del encoded_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpdjBB9fmbu2"
   },
   "source": [
    "Preprocess and tokenize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31755,
     "status": "ok",
     "timestamp": 1604426004546,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "QTlQzTzAfCy7",
    "outputId": "cb02b1ce-0861-46c4-8c33-60fe5017e06c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  apareceu o √≠ndice de morte na minha cidade t√¥ muito assustado #BelemPedePaz\n",
      "Token IDs:  [101, 4169, 146, 2884, 22279, 125, 1386, 229, 7122, 651, 374, 785, 17154, 487, 108, 4826, 21813, 11237, 321, 22305, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Encode the first dataset sentence and show Token IDs\n",
    "token_ids = list(preprocess([X_train[0]], **preprocessing_params)[0].squeeze().numpy())\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "train_inputs, train_masks = preprocess(X_train, **preprocessing_params)\n",
    "test_inputs, test_masks = preprocess(X_test, **preprocessing_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZU8t5VNfvhY"
   },
   "source": [
    "### Create PyTorch DataLoaders for Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 31751,
     "status": "ok",
     "timestamp": 1604426004553,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "xHuYEc61gcGL"
   },
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(y_train, dtype=torch.int64)\n",
    "test_labels = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSRAga-yj17q"
   },
   "source": [
    "## Train\n",
    "\n",
    "Define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 31716,
     "status": "ok",
     "timestamp": 1604426004559,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "Xy4HkhyECibW"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, scheduler, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \n",
    "    best_eval = 0.\n",
    "    best_epoch = -1.\n",
    "    best_model_state = None\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val F1':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(get_device()) for t in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "                loss = loss_fn(logits, b_labels)\n",
    "                batch_loss += loss.item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip the norm of the gradients to 1.0\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scheduler.step()\n",
    "            scaler.update()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\" * 70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            val_loss, val_f1 = evaluate(model, val_dataloader, loss_fn)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_f1:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 70)\n",
    "            if val_f1 > best_eval:\n",
    "                best_eval = val_f1\n",
    "                best_epoch = epoch_i\n",
    "                torch.save(model.state_dict(), '../artifacts/models/best-model-parameters.pt')\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(f\"Training complete. Best result: {best_eval} | epoch {best_epoch}.\")\n",
    "    model.load_state_dict(torch.load('../artifacts/models/best-model-parameters.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSfTy9LqiFD-"
   },
   "source": [
    "Training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150245,
     "status": "ok",
     "timestamp": 1604426123094,
     "user": {
      "displayName": "Bruno Vilar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitnJqA4yg0YW7lBxpd6Acfxpcpe2vurX_VBhN1Kw=s64",
      "userId": "12191660000722192802"
     },
     "user_tz": 180
    },
    "id": "wfYw7dJ0U0v6",
    "outputId": "7d9edb7f-d820-49d7-aec4-fd2099916696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/bruno/f331903b-17cb-447b-b132-e6f1f08f80f8/Development/02_TwitterBR_SentimentAnalysis/venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/media/bruno/f331903b-17cb-447b-b132-e6f1f08f80f8/Development/02_TwitterBR_SentimentAnalysis/venv/lib/python3.8/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   20    |   1.101604   |     -      |     -     |   2.27   \n",
      "   1    |   40    |   1.070011   |     -      |     -     |   2.15   \n",
      "   1    |   60    |   1.030616   |     -      |     -     |   2.15   \n",
      "   1    |   80    |   1.018616   |     -      |     -     |   2.22   \n",
      "   1    |   100   |   0.841594   |     -      |     -     |   2.15   \n",
      "   1    |   120   |   0.825158   |     -      |     -     |   2.15   \n",
      "   1    |   140   |   0.748860   |     -      |     -     |   2.14   \n",
      "   1    |   160   |   0.757830   |     -      |     -     |   2.25   \n",
      "   1    |   180   |   0.649466   |     -      |     -     |   2.24   \n",
      "   1    |   200   |   0.913589   |     -      |     -     |   2.30   \n",
      "   1    |   220   |   0.749083   |     -      |     -     |   2.25   \n",
      "   1    |   240   |   0.819263   |     -      |     -     |   2.23   \n",
      "   1    |   260   |   0.822560   |     -      |     -     |   2.23   \n",
      "   1    |   280   |   0.773651   |     -      |     -     |   2.21   \n",
      "   1    |   300   |   0.777709   |     -      |     -     |   2.20   \n",
      "   1    |   320   |   0.801476   |     -      |     -     |   2.15   \n",
      "   1    |   340   |   0.690222   |     -      |     -     |   2.24   \n",
      "   1    |   360   |   0.814000   |     -      |     -     |   2.22   \n",
      "   1    |   380   |   0.765515   |     -      |     -     |   2.19   \n",
      "   1    |   400   |   0.726191   |     -      |     -     |   2.20   \n",
      "   1    |   420   |   0.822051   |     -      |     -     |   2.24   \n",
      "   1    |   440   |   0.737018   |     -      |     -     |   2.27   \n",
      "   1    |   460   |   0.624137   |     -      |     -     |   2.26   \n",
      "   1    |   480   |   0.673063   |     -      |     -     |   2.26   \n",
      "   1    |   500   |   0.833435   |     -      |     -     |   2.23   \n",
      "   1    |   520   |   0.768449   |     -      |     -     |   2.21   \n",
      "   1    |   540   |   0.771972   |     -      |     -     |   2.21   \n",
      "   1    |   560   |   0.756395   |     -      |     -     |   2.24   \n",
      "   1    |   580   |   0.851794   |     -      |     -     |   2.24   \n",
      "   1    |   600   |   0.604018   |     -      |     -     |   2.23   \n",
      "   1    |   620   |   0.651204   |     -      |     -     |   2.17   \n",
      "   1    |   640   |   0.724065   |     -      |     -     |   2.17   \n",
      "   1    |   660   |   0.722230   |     -      |     -     |   2.18   \n",
      "   1    |   680   |   0.709388   |     -      |     -     |   2.23   \n",
      "   1    |   700   |   0.791626   |     -      |     -     |   2.19   \n",
      "   1    |   720   |   0.707713   |     -      |     -     |   2.19   \n",
      "   1    |   740   |   0.768871   |     -      |     -     |   2.20   \n",
      "   1    |   760   |   0.672045   |     -      |     -     |   2.26   \n",
      "   1    |   780   |   0.833346   |     -      |     -     |   2.27   \n",
      "   1    |   800   |   0.696689   |     -      |     -     |   2.22   \n",
      "   1    |   820   |   0.684665   |     -      |     -     |   2.23   \n",
      "   1    |   840   |   0.772103   |     -      |     -     |   2.22   \n",
      "   1    |   860   |   0.737577   |     -      |     -     |   2.23   \n",
      "   1    |   880   |   0.648416   |     -      |     -     |   2.25   \n",
      "   1    |   900   |   0.706232   |     -      |     -     |   2.25   \n",
      "   1    |   920   |   0.734979   |     -      |     -     |   2.24   \n",
      "   1    |   940   |   0.689647   |     -      |     -     |   2.22   \n",
      "   1    |   960   |   0.770124   |     -      |     -     |   2.21   \n",
      "   1    |   980   |   0.710719   |     -      |     -     |   2.23   \n",
      "   1    |  1000   |   0.659342   |     -      |     -     |   2.19   \n",
      "   1    |  1020   |   0.716747   |     -      |     -     |   2.20   \n",
      "   1    |  1040   |   0.784489   |     -      |     -     |   2.17   \n",
      "   1    |  1060   |   0.655834   |     -      |     -     |   2.22   \n",
      "   1    |  1080   |   0.591440   |     -      |     -     |   2.20   \n",
      "   1    |  1100   |   0.775673   |     -      |     -     |   2.22   \n",
      "   1    |  1120   |   0.751502   |     -      |     -     |   2.19   \n",
      "   1    |  1140   |   0.644934   |     -      |     -     |   2.22   \n",
      "   1    |  1160   |   0.807059   |     -      |     -     |   2.15   \n",
      "   1    |  1180   |   0.618325   |     -      |     -     |   2.13   \n",
      "   1    |  1200   |   0.748776   |     -      |     -     |   2.19   \n",
      "   1    |  1220   |   0.641853   |     -      |     -     |   2.21   \n",
      "   1    |  1240   |   0.654969   |     -      |     -     |   2.18   \n",
      "   1    |  1260   |   0.699223   |     -      |     -     |   2.16   \n",
      "   1    |  1280   |   0.666192   |     -      |     -     |   2.19   \n",
      "   1    |  1300   |   0.727381   |     -      |     -     |   2.21   \n",
      "   1    |  1320   |   0.676563   |     -      |     -     |   2.21   \n",
      "   1    |  1340   |   0.727606   |     -      |     -     |   2.21   \n",
      "   1    |  1360   |   0.646141   |     -      |     -     |   2.21   \n",
      "   1    |  1380   |   0.744940   |     -      |     -     |   2.18   \n",
      "   1    |  1400   |   0.676654   |     -      |     -     |   2.22   \n",
      "   1    |  1420   |   0.647859   |     -      |     -     |   2.21   \n",
      "   1    |  1440   |   0.709311   |     -      |     -     |   2.24   \n",
      "   1    |  1460   |   0.587732   |     -      |     -     |   2.23   \n",
      "   1    |  1480   |   0.666570   |     -      |     -     |   2.20   \n",
      "   1    |  1500   |   0.740993   |     -      |     -     |   2.21   \n",
      "   1    |  1520   |   0.753292   |     -      |     -     |   2.19   \n",
      "   1    |  1540   |   0.729853   |     -      |     -     |   2.21   \n",
      "   1    |  1560   |   0.686162   |     -      |     -     |   2.20   \n",
      "   1    |  1580   |   0.593731   |     -      |     -     |   2.19   \n",
      "   1    |  1600   |   0.751720   |     -      |     -     |   2.19   \n",
      "   1    |  1620   |   0.851061   |     -      |     -     |   2.26   \n",
      "   1    |  1623   |   0.609687   |     -      |     -     |   0.34   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.743244   |  0.562514  |   76.88   |  185.27  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.477016   |     -      |     -     |   2.31   \n",
      "   2    |   40    |   0.500852   |     -      |     -     |   2.20   \n",
      "   2    |   60    |   0.394667   |     -      |     -     |   2.21   \n",
      "   2    |   80    |   0.758917   |     -      |     -     |   2.20   \n",
      "   2    |   100   |   0.532754   |     -      |     -     |   2.20   \n",
      "   2    |   120   |   0.623580   |     -      |     -     |   2.22   \n",
      "   2    |   140   |   0.425376   |     -      |     -     |   2.20   \n",
      "   2    |   160   |   0.575062   |     -      |     -     |   2.18   \n",
      "   2    |   180   |   0.502239   |     -      |     -     |   2.20   \n",
      "   2    |   200   |   0.487223   |     -      |     -     |   2.17   \n",
      "   2    |   220   |   0.492480   |     -      |     -     |   2.22   \n",
      "   2    |   240   |   0.436340   |     -      |     -     |   2.19   \n",
      "   2    |   260   |   0.591772   |     -      |     -     |   2.16   \n",
      "   2    |   280   |   0.568689   |     -      |     -     |   2.16   \n",
      "   2    |   300   |   0.528446   |     -      |     -     |   2.15   \n",
      "   2    |   320   |   0.483338   |     -      |     -     |   2.16   \n",
      "   2    |   340   |   0.593444   |     -      |     -     |   2.19   \n",
      "   2    |   360   |   0.528742   |     -      |     -     |   2.18   \n",
      "   2    |   380   |   0.493898   |     -      |     -     |   2.23   \n",
      "   2    |   400   |   0.566369   |     -      |     -     |   2.18   \n",
      "   2    |   420   |   0.474370   |     -      |     -     |   2.21   \n",
      "   2    |   440   |   0.549065   |     -      |     -     |   2.17   \n",
      "   2    |   460   |   0.534341   |     -      |     -     |   2.15   \n",
      "   2    |   480   |   0.491904   |     -      |     -     |   2.23   \n",
      "   2    |   500   |   0.491988   |     -      |     -     |   2.20   \n",
      "   2    |   520   |   0.488502   |     -      |     -     |   2.18   \n",
      "   2    |   540   |   0.473478   |     -      |     -     |   2.15   \n",
      "   2    |   560   |   0.410110   |     -      |     -     |   2.17   \n",
      "   2    |   580   |   0.562996   |     -      |     -     |   2.19   \n",
      "   2    |   600   |   0.379525   |     -      |     -     |   2.18   \n",
      "   2    |   620   |   0.419996   |     -      |     -     |   2.19   \n",
      "   2    |   640   |   0.467457   |     -      |     -     |   2.19   \n",
      "   2    |   660   |   0.512947   |     -      |     -     |   2.16   \n",
      "   2    |   680   |   0.547419   |     -      |     -     |   2.18   \n",
      "   2    |   700   |   0.451840   |     -      |     -     |   2.20   \n",
      "   2    |   720   |   0.324617   |     -      |     -     |   2.18   \n",
      "   2    |   740   |   0.384184   |     -      |     -     |   2.19   \n",
      "   2    |   760   |   0.518907   |     -      |     -     |   2.18   \n",
      "   2    |   780   |   0.455850   |     -      |     -     |   2.21   \n",
      "   2    |   800   |   0.513845   |     -      |     -     |   2.18   \n",
      "   2    |   820   |   0.612065   |     -      |     -     |   2.21   \n",
      "   2    |   840   |   0.508731   |     -      |     -     |   2.16   \n",
      "   2    |   860   |   0.440456   |     -      |     -     |   2.23   \n",
      "   2    |   880   |   0.421236   |     -      |     -     |   2.20   \n",
      "   2    |   900   |   0.474186   |     -      |     -     |   2.17   \n",
      "   2    |   920   |   0.466604   |     -      |     -     |   2.16   \n",
      "   2    |   940   |   0.478513   |     -      |     -     |   2.17   \n",
      "   2    |   960   |   0.482256   |     -      |     -     |   2.23   \n",
      "   2    |   980   |   0.449716   |     -      |     -     |   2.21   \n",
      "   2    |  1000   |   0.575706   |     -      |     -     |   2.23   \n",
      "   2    |  1020   |   0.455915   |     -      |     -     |   2.23   \n",
      "   2    |  1040   |   0.462929   |     -      |     -     |   2.19   \n",
      "   2    |  1060   |   0.543054   |     -      |     -     |   2.18   \n",
      "   2    |  1080   |   0.436021   |     -      |     -     |   2.21   \n",
      "   2    |  1100   |   0.500530   |     -      |     -     |   2.20   \n",
      "   2    |  1120   |   0.374150   |     -      |     -     |   2.19   \n",
      "   2    |  1140   |   0.416830   |     -      |     -     |   2.20   \n",
      "   2    |  1160   |   0.321721   |     -      |     -     |   2.23   \n",
      "   2    |  1180   |   0.470382   |     -      |     -     |   2.22   \n",
      "   2    |  1200   |   0.486664   |     -      |     -     |   2.24   \n",
      "   2    |  1220   |   0.548271   |     -      |     -     |   2.24   \n",
      "   2    |  1240   |   0.551710   |     -      |     -     |   2.24   \n",
      "   2    |  1260   |   0.496686   |     -      |     -     |   2.24   \n",
      "   2    |  1280   |   0.433207   |     -      |     -     |   2.25   \n",
      "   2    |  1300   |   0.451216   |     -      |     -     |   2.25   \n",
      "   2    |  1320   |   0.462264   |     -      |     -     |   2.25   \n",
      "   2    |  1340   |   0.442755   |     -      |     -     |   2.24   \n",
      "   2    |  1360   |   0.470370   |     -      |     -     |   2.23   \n",
      "   2    |  1380   |   0.368423   |     -      |     -     |   2.19   \n",
      "   2    |  1400   |   0.388421   |     -      |     -     |   2.18   \n",
      "   2    |  1420   |   0.469850   |     -      |     -     |   2.16   \n",
      "   2    |  1440   |   0.515516   |     -      |     -     |   2.17   \n",
      "   2    |  1460   |   0.472839   |     -      |     -     |   2.15   \n",
      "   2    |  1480   |   0.465100   |     -      |     -     |   2.12   \n",
      "   2    |  1500   |   0.552204   |     -      |     -     |   2.12   \n",
      "   2    |  1520   |   0.440103   |     -      |     -     |   2.13   \n",
      "   2    |  1540   |   0.484842   |     -      |     -     |   2.13   \n",
      "   2    |  1560   |   0.503209   |     -      |     -     |   2.12   \n",
      "   2    |  1580   |   0.442974   |     -      |     -     |   2.14   \n",
      "   2    |  1600   |   0.478302   |     -      |     -     |   2.21   \n",
      "   2    |  1620   |   0.459298   |     -      |     -     |   2.19   \n",
      "   2    |  1623   |   0.647310   |     -      |     -     |   0.33   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.485354   |  0.614066  |   76.44   |  183.81  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.391507   |     -      |     -     |   2.26   \n",
      "   3    |   40    |   0.296740   |     -      |     -     |   2.16   \n",
      "   3    |   60    |   0.233111   |     -      |     -     |   2.15   \n",
      "   3    |   80    |   0.190856   |     -      |     -     |   2.40   \n",
      "   3    |   100   |   0.424319   |     -      |     -     |   2.40   \n",
      "   3    |   120   |   0.439439   |     -      |     -     |   2.26   \n",
      "   3    |   140   |   0.356817   |     -      |     -     |   2.38   \n",
      "   3    |   160   |   0.283044   |     -      |     -     |   2.32   \n",
      "   3    |   180   |   0.337565   |     -      |     -     |   2.28   \n",
      "   3    |   200   |   0.350990   |     -      |     -     |   2.40   \n",
      "   3    |   220   |   0.325796   |     -      |     -     |   2.29   \n",
      "   3    |   240   |   0.481986   |     -      |     -     |   2.31   \n",
      "   3    |   260   |   0.465382   |     -      |     -     |   2.30   \n",
      "   3    |   280   |   0.354700   |     -      |     -     |   2.41   \n",
      "   3    |   300   |   0.370259   |     -      |     -     |   2.28   \n",
      "   3    |   320   |   0.254840   |     -      |     -     |   2.33   \n",
      "   3    |   340   |   0.468593   |     -      |     -     |   2.27   \n",
      "   3    |   360   |   0.367463   |     -      |     -     |   2.29   \n",
      "   3    |   380   |   0.521577   |     -      |     -     |   2.33   \n",
      "   3    |   400   |   0.340876   |     -      |     -     |   2.26   \n",
      "   3    |   420   |   0.213498   |     -      |     -     |   2.23   \n",
      "   3    |   440   |   0.418337   |     -      |     -     |   2.33   \n",
      "   3    |   460   |   0.242285   |     -      |     -     |   2.35   \n",
      "   3    |   480   |   0.403658   |     -      |     -     |   2.32   \n",
      "   3    |   500   |   0.501469   |     -      |     -     |   2.38   \n",
      "   3    |   520   |   0.402722   |     -      |     -     |   2.33   \n",
      "   3    |   540   |   0.401317   |     -      |     -     |   2.34   \n",
      "   3    |   560   |   0.392679   |     -      |     -     |   2.33   \n",
      "   3    |   580   |   0.489702   |     -      |     -     |   2.28   \n",
      "   3    |   600   |   0.270927   |     -      |     -     |   2.35   \n",
      "   3    |   620   |   0.442032   |     -      |     -     |   2.23   \n",
      "   3    |   640   |   0.514853   |     -      |     -     |   2.25   \n",
      "   3    |   660   |   0.441201   |     -      |     -     |   2.37   \n",
      "   3    |   680   |   0.368711   |     -      |     -     |   2.16   \n",
      "   3    |   700   |   0.284315   |     -      |     -     |   2.32   \n",
      "   3    |   720   |   0.434365   |     -      |     -     |   2.20   \n",
      "   3    |   740   |   0.234702   |     -      |     -     |   2.31   \n",
      "   3    |   760   |   0.355038   |     -      |     -     |   2.34   \n",
      "   3    |   780   |   0.466919   |     -      |     -     |   2.32   \n",
      "   3    |   800   |   0.447277   |     -      |     -     |   2.39   \n",
      "   3    |   820   |   0.542368   |     -      |     -     |   2.35   \n",
      "   3    |   840   |   0.326442   |     -      |     -     |   2.25   \n",
      "   3    |   860   |   0.493193   |     -      |     -     |   2.30   \n",
      "   3    |   880   |   0.449010   |     -      |     -     |   2.35   \n",
      "   3    |   900   |   0.267428   |     -      |     -     |   2.35   \n",
      "   3    |   920   |   0.283283   |     -      |     -     |   2.37   \n",
      "   3    |   940   |   0.313448   |     -      |     -     |   2.36   \n",
      "   3    |   960   |   0.381300   |     -      |     -     |   2.25   \n",
      "   3    |   980   |   0.370400   |     -      |     -     |   2.26   \n",
      "   3    |  1000   |   0.334371   |     -      |     -     |   2.32   \n",
      "   3    |  1020   |   0.207541   |     -      |     -     |   2.23   \n",
      "   3    |  1040   |   0.567996   |     -      |     -     |   2.20   \n",
      "   3    |  1060   |   0.349020   |     -      |     -     |   2.24   \n",
      "   3    |  1080   |   0.537219   |     -      |     -     |   2.36   \n",
      "   3    |  1100   |   0.404950   |     -      |     -     |   2.26   \n",
      "   3    |  1120   |   0.406547   |     -      |     -     |   2.34   \n",
      "   3    |  1140   |   0.342676   |     -      |     -     |   2.37   \n",
      "   3    |  1160   |   0.558537   |     -      |     -     |   2.38   \n",
      "   3    |  1180   |   0.447662   |     -      |     -     |   2.33   \n",
      "   3    |  1200   |   0.288760   |     -      |     -     |   2.21   \n",
      "   3    |  1220   |   0.276410   |     -      |     -     |   2.27   \n",
      "   3    |  1240   |   0.369666   |     -      |     -     |   2.29   \n",
      "   3    |  1260   |   0.385416   |     -      |     -     |   2.35   \n",
      "   3    |  1280   |   0.234655   |     -      |     -     |   2.20   \n",
      "   3    |  1300   |   0.367997   |     -      |     -     |   2.17   \n",
      "   3    |  1320   |   0.307928   |     -      |     -     |   2.17   \n",
      "   3    |  1340   |   0.380750   |     -      |     -     |   2.19   \n",
      "   3    |  1360   |   0.318210   |     -      |     -     |   2.28   \n",
      "   3    |  1380   |   0.247686   |     -      |     -     |   2.37   \n",
      "   3    |  1400   |   0.402132   |     -      |     -     |   2.31   \n",
      "   3    |  1420   |   0.513506   |     -      |     -     |   2.26   \n",
      "   3    |  1440   |   0.420841   |     -      |     -     |   2.37   \n",
      "   3    |  1460   |   0.416395   |     -      |     -     |   2.27   \n",
      "   3    |  1480   |   0.417703   |     -      |     -     |   2.36   \n",
      "   3    |  1500   |   0.420411   |     -      |     -     |   2.25   \n",
      "   3    |  1520   |   0.409655   |     -      |     -     |   2.33   \n",
      "   3    |  1540   |   0.410481   |     -      |     -     |   2.39   \n",
      "   3    |  1560   |   0.551070   |     -      |     -     |   2.32   \n",
      "   3    |  1580   |   0.588709   |     -      |     -     |   2.39   \n",
      "   3    |  1600   |   0.265311   |     -      |     -     |   2.37   \n",
      "   3    |  1620   |   0.605602   |     -      |     -     |   2.37   \n",
      "   3    |  1623   |   0.759186   |     -      |     -     |   0.34   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.385443   |  1.496388  |   76.39   |  192.99  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.262906   |     -      |     -     |   2.33   \n",
      "   4    |   40    |   0.237093   |     -      |     -     |   2.20   \n",
      "   4    |   60    |   0.425287   |     -      |     -     |   2.18   \n",
      "   4    |   80    |   0.269063   |     -      |     -     |   2.21   \n",
      "   4    |   100   |   0.248103   |     -      |     -     |   2.23   \n",
      "   4    |   120   |   0.203419   |     -      |     -     |   2.17   \n",
      "   4    |   140   |   0.210589   |     -      |     -     |   2.18   \n",
      "   4    |   160   |   0.145793   |     -      |     -     |   2.18   \n",
      "   4    |   180   |   0.079305   |     -      |     -     |   2.19   \n",
      "   4    |   200   |   0.054954   |     -      |     -     |   2.18   \n",
      "   4    |   220   |   0.331212   |     -      |     -     |   2.31   \n",
      "   4    |   240   |   0.421453   |     -      |     -     |   2.29   \n",
      "   4    |   260   |   0.348248   |     -      |     -     |   2.26   \n",
      "   4    |   280   |   0.460681   |     -      |     -     |   2.38   \n",
      "   4    |   300   |   0.425819   |     -      |     -     |   2.26   \n",
      "   4    |   320   |   0.367751   |     -      |     -     |   2.19   \n",
      "   4    |   340   |   0.312452   |     -      |     -     |   2.40   \n",
      "   4    |   360   |   0.426052   |     -      |     -     |   2.40   \n",
      "   4    |   380   |   0.247630   |     -      |     -     |   2.40   \n",
      "   4    |   400   |   0.416190   |     -      |     -     |   2.31   \n",
      "   4    |   420   |   0.112481   |     -      |     -     |   2.35   \n",
      "   4    |   440   |   0.298633   |     -      |     -     |   2.25   \n",
      "   4    |   460   |   0.330460   |     -      |     -     |   2.30   \n",
      "   4    |   480   |   0.526627   |     -      |     -     |   2.25   \n",
      "   4    |   500   |   0.279508   |     -      |     -     |   2.28   \n",
      "   4    |   520   |   0.511597   |     -      |     -     |   2.33   \n",
      "   4    |   540   |   0.350836   |     -      |     -     |   2.21   \n",
      "   4    |   560   |   0.460040   |     -      |     -     |   2.17   \n",
      "   4    |   580   |   0.305329   |     -      |     -     |   2.18   \n",
      "   4    |   600   |   0.430295   |     -      |     -     |   2.17   \n",
      "   4    |   620   |   0.423888   |     -      |     -     |   2.16   \n",
      "   4    |   640   |   0.851877   |     -      |     -     |   2.29   \n",
      "   4    |   660   |   0.280808   |     -      |     -     |   2.33   \n",
      "   4    |   680   |   0.578305   |     -      |     -     |   2.30   \n",
      "   4    |   700   |   0.470845   |     -      |     -     |   2.18   \n",
      "   4    |   720   |   0.496206   |     -      |     -     |   2.20   \n",
      "   4    |   740   |   0.702803   |     -      |     -     |   2.18   \n",
      "   4    |   760   |   0.415619   |     -      |     -     |   2.25   \n",
      "   4    |   780   |   0.441995   |     -      |     -     |   2.25   \n",
      "   4    |   800   |   0.409622   |     -      |     -     |   2.25   \n",
      "   4    |   820   |   0.531428   |     -      |     -     |   2.31   \n",
      "   4    |   840   |   0.588306   |     -      |     -     |   2.24   \n",
      "   4    |   860   |   0.795919   |     -      |     -     |   2.33   \n",
      "   4    |   880   |   0.416197   |     -      |     -     |   2.22   \n",
      "   4    |   900   |   0.507387   |     -      |     -     |   2.26   \n",
      "   4    |   920   |   0.598558   |     -      |     -     |   2.27   \n",
      "   4    |   940   |   0.216686   |     -      |     -     |   2.31   \n",
      "   4    |   960   |   0.757328   |     -      |     -     |   2.28   \n",
      "   4    |   980   |   0.869422   |     -      |     -     |   2.27   \n",
      "   4    |  1000   |   0.225066   |     -      |     -     |   2.25   \n",
      "   4    |  1020   |   0.260041   |     -      |     -     |   2.22   \n",
      "   4    |  1040   |   0.456319   |     -      |     -     |   2.27   \n",
      "   4    |  1060   |   0.485191   |     -      |     -     |   2.28   \n",
      "   4    |  1080   |   0.442945   |     -      |     -     |   2.31   \n",
      "   4    |  1100   |   0.322888   |     -      |     -     |   2.32   \n",
      "   4    |  1120   |   0.762953   |     -      |     -     |   2.27   \n",
      "   4    |  1140   |   0.836831   |     -      |     -     |   2.28   \n",
      "   4    |  1160   |   0.383159   |     -      |     -     |   2.34   \n",
      "   4    |  1180   |   0.225752   |     -      |     -     |   2.25   \n",
      "   4    |  1200   |   0.532704   |     -      |     -     |   2.28   \n",
      "   4    |  1220   |   0.648933   |     -      |     -     |   2.31   \n",
      "   4    |  1240   |   0.796624   |     -      |     -     |   2.31   \n",
      "   4    |  1260   |   0.290485   |     -      |     -     |   2.37   \n",
      "   4    |  1280   |   0.498636   |     -      |     -     |   2.31   \n",
      "   4    |  1300   |   0.510091   |     -      |     -     |   2.23   \n",
      "   4    |  1320   |   0.304630   |     -      |     -     |   2.25   \n",
      "   4    |  1340   |   0.608271   |     -      |     -     |   2.25   \n",
      "   4    |  1360   |   0.384910   |     -      |     -     |   2.27   \n",
      "   4    |  1380   |   0.492933   |     -      |     -     |   2.25   \n",
      "   4    |  1400   |   0.362251   |     -      |     -     |   2.25   \n",
      "   4    |  1420   |   0.437865   |     -      |     -     |   2.25   \n",
      "   4    |  1440   |   0.473722   |     -      |     -     |   2.27   \n",
      "   4    |  1460   |   0.714678   |     -      |     -     |   2.26   \n",
      "   4    |  1480   |   0.520872   |     -      |     -     |   2.27   \n",
      "   4    |  1500   |   0.627642   |     -      |     -     |   2.22   \n",
      "   4    |  1520   |   0.280439   |     -      |     -     |   2.21   \n",
      "   4    |  1540   |   0.361813   |     -      |     -     |   2.29   \n",
      "   4    |  1560   |   0.522439   |     -      |     -     |   2.37   \n",
      "   4    |  1580   |   0.667868   |     -      |     -     |   2.34   \n",
      "   4    |  1600   |   0.661517   |     -      |     -     |   2.29   \n",
      "   4    |  1620   |   0.527691   |     -      |     -     |   2.27   \n",
      "   4    |  1623   |   0.014850   |     -      |     -     |   0.35   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.437150   |  2.451824  |   75.94   |  190.08  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   20    |   0.296748   |     -      |     -     |   2.37   \n",
      "   5    |   40    |   0.261366   |     -      |     -     |   2.28   \n",
      "   5    |   60    |   0.166662   |     -      |     -     |   2.21   \n",
      "   5    |   80    |   0.306957   |     -      |     -     |   2.21   \n",
      "   5    |   100   |   0.337134   |     -      |     -     |   2.24   \n",
      "   5    |   120   |   0.392594   |     -      |     -     |   2.29   \n",
      "   5    |   140   |   0.280197   |     -      |     -     |   2.25   \n",
      "   5    |   160   |   0.217916   |     -      |     -     |   2.34   \n",
      "   5    |   180   |   0.197637   |     -      |     -     |   2.19   \n",
      "   5    |   200   |   0.203415   |     -      |     -     |   2.31   \n",
      "   5    |   220   |   0.213178   |     -      |     -     |   2.29   \n",
      "   5    |   240   |   0.591552   |     -      |     -     |   2.19   \n",
      "   5    |   260   |   0.431123   |     -      |     -     |   2.33   \n",
      "   5    |   280   |   0.142357   |     -      |     -     |   2.15   \n",
      "   5    |   300   |   0.826412   |     -      |     -     |   2.20   \n",
      "   5    |   320   |   0.265512   |     -      |     -     |   2.19   \n",
      "   5    |   340   |   0.347550   |     -      |     -     |   2.21   \n",
      "   5    |   360   |   0.395217   |     -      |     -     |   2.19   \n",
      "   5    |   380   |   0.391674   |     -      |     -     |   2.29   \n",
      "   5    |   400   |   0.230693   |     -      |     -     |   2.26   \n",
      "   5    |   420   |   0.243127   |     -      |     -     |   2.23   \n",
      "   5    |   440   |   0.357583   |     -      |     -     |   2.27   \n",
      "   5    |   460   |   0.206898   |     -      |     -     |   2.27   \n",
      "   5    |   480   |   0.249900   |     -      |     -     |   2.34   \n",
      "   5    |   500   |   0.105398   |     -      |     -     |   2.34   \n",
      "   5    |   520   |   0.255834   |     -      |     -     |   2.25   \n",
      "   5    |   540   |   0.347155   |     -      |     -     |   2.33   \n",
      "   5    |   560   |   0.286433   |     -      |     -     |   2.24   \n",
      "   5    |   580   |   0.313216   |     -      |     -     |   2.27   \n",
      "   5    |   600   |   0.468786   |     -      |     -     |   2.26   \n",
      "   5    |   620   |   0.513017   |     -      |     -     |   2.25   \n",
      "   5    |   640   |   0.063436   |     -      |     -     |   2.19   \n",
      "   5    |   660   |   0.389673   |     -      |     -     |   2.21   \n",
      "   5    |   680   |   0.214366   |     -      |     -     |   2.21   \n",
      "   5    |   700   |   0.213191   |     -      |     -     |   2.21   \n",
      "   5    |   720   |   0.293866   |     -      |     -     |   2.24   \n",
      "   5    |   740   |   0.318929   |     -      |     -     |   2.21   \n",
      "   5    |   760   |   0.533927   |     -      |     -     |   2.26   \n",
      "   5    |   780   |   0.338583   |     -      |     -     |   2.26   \n",
      "   5    |   800   |   0.206024   |     -      |     -     |   2.23   \n",
      "   5    |   820   |   0.227213   |     -      |     -     |   2.20   \n",
      "   5    |   840   |   0.461503   |     -      |     -     |   2.22   \n",
      "   5    |   860   |   0.195926   |     -      |     -     |   2.18   \n",
      "   5    |   880   |   0.311548   |     -      |     -     |   2.19   \n",
      "   5    |   900   |   0.353169   |     -      |     -     |   2.19   \n",
      "   5    |   920   |   0.328378   |     -      |     -     |   2.22   \n",
      "   5    |   940   |   0.159472   |     -      |     -     |   2.18   \n",
      "   5    |   960   |   0.268070   |     -      |     -     |   2.19   \n",
      "   5    |   980   |   0.243032   |     -      |     -     |   2.21   \n",
      "   5    |  1000   |   0.203730   |     -      |     -     |   2.19   \n",
      "   5    |  1020   |   0.418567   |     -      |     -     |   2.19   \n",
      "   5    |  1040   |   0.481290   |     -      |     -     |   2.18   \n",
      "   5    |  1060   |   0.199113   |     -      |     -     |   2.20   \n",
      "   5    |  1080   |   0.101542   |     -      |     -     |   2.13   \n",
      "   5    |  1100   |   0.148863   |     -      |     -     |   2.13   \n",
      "   5    |  1120   |   0.297979   |     -      |     -     |   2.14   \n",
      "   5    |  1140   |   0.594321   |     -      |     -     |   2.15   \n",
      "   5    |  1160   |   0.185321   |     -      |     -     |   2.16   \n",
      "   5    |  1180   |   0.425349   |     -      |     -     |   2.21   \n",
      "   5    |  1200   |   0.216801   |     -      |     -     |   2.18   \n",
      "   5    |  1220   |   0.301962   |     -      |     -     |   2.17   \n",
      "   5    |  1240   |   0.295611   |     -      |     -     |   2.18   \n",
      "   5    |  1260   |   0.345284   |     -      |     -     |   2.14   \n",
      "   5    |  1280   |   0.238935   |     -      |     -     |   2.12   \n",
      "   5    |  1300   |   0.281269   |     -      |     -     |   2.14   \n",
      "   5    |  1320   |   0.468141   |     -      |     -     |   2.13   \n",
      "   5    |  1340   |   0.488479   |     -      |     -     |   2.14   \n",
      "   5    |  1360   |   0.454589   |     -      |     -     |   2.23   \n",
      "   5    |  1380   |   0.265397   |     -      |     -     |   2.20   \n",
      "   5    |  1400   |   0.260903   |     -      |     -     |   2.24   \n",
      "   5    |  1420   |   0.258063   |     -      |     -     |   2.19   \n",
      "   5    |  1440   |   0.358827   |     -      |     -     |   2.18   \n",
      "   5    |  1460   |   0.415424   |     -      |     -     |   2.20   \n",
      "   5    |  1480   |   0.229698   |     -      |     -     |   2.20   \n",
      "   5    |  1500   |   0.429563   |     -      |     -     |   2.15   \n",
      "   5    |  1520   |   0.104973   |     -      |     -     |   2.10   \n",
      "   5    |  1540   |   0.197133   |     -      |     -     |   2.16   \n",
      "   5    |  1560   |   0.294082   |     -      |     -     |   2.17   \n",
      "   5    |  1580   |   0.273089   |     -      |     -     |   2.17   \n",
      "   5    |  1600   |   0.473937   |     -      |     -     |   2.17   \n",
      "   5    |  1620   |   0.185377   |     -      |     -     |   2.15   \n",
      "   5    |  1623   |   0.877451   |     -      |     -     |   0.33   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.307926   |  2.793445  |   76.19   |  185.48  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete. Best result: 76.88492063492063 | epoch 0.\n"
     ]
    }
   ],
   "source": [
    "set_seed(SEED)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "transformer_classifier, optimizer, scheduler = initialize_model(MODEL_CLASS, MODEL_NAME, \n",
    "                                                                MODEL_LAYERS, MODEL_DROPOUT_LAYERS,\n",
    "                                                                len(train_dataloader), epochs=EPOCHS, \n",
    "                                                                freeze=FREEZE, learning_rate=LEARNING_RATE)\n",
    "\n",
    "execution_params = {\n",
    "    'model_class':MODEL_CLASS,\n",
    "    'model_tokenizer': MODEL_TOKENIZER,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'model_layers': MODEL_LAYERS,\n",
    "    'model_dropout_layers': MODEL_DROPOUT_LAYERS,\n",
    "    'freeze': FREEZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'model_definition': transformer_classifier.classifier,\n",
    "    'scheduler': scheduler,\n",
    "    'optimizer': optimizer,\n",
    "    'seed': SEED,\n",
    "    'device': get_device()\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=EXPERIMENT_RUN_NAME, experiment_id=EXPERIMENT_ID) as main_run:\n",
    "    mlflow.log_params(format_nested_parameters(preprocessing_params, 'preprocessing'))\n",
    "    mlflow.log_params(format_nested_parameters(execution_params, 'execution'))\n",
    "    \n",
    "    mlflow.log_param('X_training', X_train.shape)\n",
    "    mlflow.log_param('X_test', X_test.shape)\n",
    "\n",
    "    start_time = time.time()\n",
    "    transformer_classifier = train(transformer_classifier, loss_fn, optimizer, scheduler, train_dataloader, \n",
    "                                   test_dataloader, epochs=EPOCHS, evaluation=True)    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    mlflow.pytorch.log_model(transformer_classifier, \"model\")\n",
    "    mlflow.log_metric('training_time', training_time)\n",
    "\n",
    "    probs = predict(transformer_classifier, test_dataloader)\n",
    "    \n",
    "    eval_metric = metrics.f1_score(y_test, probs.argmax(axis=1), average=None)\n",
    "    global_eval_metric = metrics.f1_score(y_test, probs.argmax(axis=1), average='micro')\n",
    "    mlflow.log_metric('F1-Measure', global_eval_metric)\n",
    "\n",
    "    evaluation_summary_frame = (pd.DataFrame([eval_metric], columns=['F1-Neg', 'F1-Neu', 'F1-Pos'])\n",
    "                                .assign(F1=global_eval_metric)\n",
    "                                [['F1-Pos', 'F1-Neu', 'F1-Neg', 'F1']]\n",
    "                               )\n",
    "    for ix, metric in enumerate(['F1-Neg', 'F1-Neu', 'F1-Pos']):\n",
    "        mlflow.log_metric(metric, evaluation_summary_frame.loc[0][metric])\n",
    "    evaluation_summary_frame['F1-Measure'] = global_eval_metric\n",
    "\n",
    "    evaluation_summary_frame.to_csv('../data/log/experiment_runs_summary.csv')\n",
    "    evaluation_summary_frame.to_html('../data/log/experiment_runs_summary.html')\n",
    "    mlflow.log_artifact('../data/log')\n",
    "\n",
    "    del train_inputs, train_masks, train_data, train_labels, train_sampler, train_dataloader\n",
    "    del test_inputs, test_masks, test_data, test_labels, test_sampler, test_dataloader\n",
    "    del optimizer, scheduler, loss_fn\n",
    "    del transformer_classifier, tokenizer, token_ids, probs\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5ostg9kPlra"
   },
   "source": [
    "## Experiments Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Test</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Pos</th>\n",
       "      <th>F1-Neu</th>\n",
       "      <th>F1-Neg</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1-Measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.845824</td>\n",
       "      <td>0.612554</td>\n",
       "      <td>0.771987</td>\n",
       "      <td>0.769652</td>\n",
       "      <td>0.769652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     F1-Pos    F1-Neu    F1-Neg        F1  F1-Measure\n",
       "0  0.845824  0.612554  0.771987  0.769652    0.769652"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h3>Test</h3>'))\n",
    "display(evaluation_summary_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Porcamente Adaptado de  BERT-for-Sentiment-Analysis.ipynb",
   "provenance": [
    {
     "file_id": "1f32gj5IYIyFipoINiC8P3DvKat-WWLUK",
     "timestamp": 1604420331516
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
